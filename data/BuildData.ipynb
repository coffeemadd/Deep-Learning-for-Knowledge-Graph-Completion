{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert embeddings into raw data\n",
    "Use the following to convert word embedding using's FaceBooks's fastText into raw data for a standard machine learning model. We are assuming a closed world solution with only positive and negative values for our 5 predicates - isChild, isSpouse, isFather, isMother, and isSibling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn as sk \n",
    "import scipy as sci \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the pickle files containing the word encodings and the entity map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = pickle.load(open('embeddings/word_vectors.pkl','rb'))\n",
    "key = pickle.load(open('processed/large_set/entities_map_large.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the files containing the positive and negative triplets of (entity,predicate,entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37395, 4), (37391, 4))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = dict.fromkeys(range(4), lambda s: s.decode('utf-8'))\n",
    "pos = np.loadtxt(r'processed/large_set/positiveTriplets_unique_large.txt',delimiter=' ',dtype=str, converters=converter)\n",
    "neg = np.loadtxt(r'processed/large_set/negativeTriplets_more_large.txt',delimiter=' ',dtype=str, converters=converter)\n",
    "pos.shape,neg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of the entities for looping over all of them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P22': 'isFather',\n",
       " 'P25': 'isMother',\n",
       " 'P26': 'isSpouse',\n",
       " 'P3373': 'isSibling',\n",
       " 'P40': 'isChild'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_key = {'P22':'isFather','P25':'isMother','P26':'isSpouse','P3373':'isSibling','P40':'isChild'}\n",
    "pred_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3195,), (5,))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = np.unique(pos[:,0])\n",
    "predicates = np.unique(pos[:,1])\n",
    "entities.shape,predicates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper functions for the main data converting tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    \"\"\"\n",
    "        Encodes a word if found in the word \n",
    "        encoding dictionary; otherwise returns\n",
    "        an empty array.\n",
    "    \"\"\"\n",
    "    if word in words.keys():\n",
    "        return(words[word])\n",
    "    max_len = max([words[x].shape[0] for x in words])\n",
    "    return(np.empty(max_len))\n",
    "\n",
    "def encode_entity(entity):\n",
    "    \"\"\"\n",
    "        Builds an entity encoding by computing\n",
    "        the average of all the word encodings.\n",
    "        Ignores any words that don't have an \n",
    "        encoding in the words_vectors.txt file.\n",
    "    \"\"\"\n",
    "    all_words = np.array([encode_word(x) for x in key[entity].split(' ') if x in words.keys()])\n",
    "    if all_words.shape[0]==0:\n",
    "        return(np.array([np.NaN]*300,dtype=float).transpose())\n",
    "    return(all_words.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary Function\n",
    "The build_rows function builds the appropriate rows of numerical data for a given entity. This is done by considering all triplets with this entity as the first object. Then it encodes all entities in this subset, including itself, and concatenates each encoding. Finally it looks as the predicate for each entity-entity pair and assigns 1,0, or -1 depending on if the predicate is true, unknown, or false for that entity-entity pair. The result is a small dataframe of all the rows of this form where the first entity is the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 607)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_rows(entity):\n",
    "    \"\"\"\n",
    "        Builds a dataframe out of the triplets\n",
    "        containing entity as the first object.\n",
    "    \"\"\"\n",
    "    encoding = encode_entity(entity)\n",
    "    temp_pos = pos[pos[:,0]==entity,:]\n",
    "    temp_neg = neg[neg[:,0]==entity,:]\n",
    "    pairs = []\n",
    "    rows = []\n",
    "    bad_encodings = []\n",
    "    for each_obj in np.concatenate([np.unique(temp_pos[:,2]),np.unique(temp_neg[:,2])]):        \n",
    "        try:\n",
    "            obj_encode = encode_entity(each_obj)\n",
    "            \n",
    "            if not all(np.isnan(obj_encode)):\n",
    "                # Get the positive & negative triplet's objects\n",
    "                temp2_pos = temp_pos[temp_pos[:,2]==each_obj,:]\n",
    "                temp2_neg = temp_neg[temp_neg[:,2]==each_obj,:]\n",
    "\n",
    "                # Build outputs from which predicates are in triplets\n",
    "                pos_preds = np.array([x in np.unique(temp2_pos[:,1]) for x in predicates],dtype=int)\n",
    "                neg_preds = np.array([x in np.unique(temp2_neg[:,1]) for x in predicates],dtype=int)\n",
    "                preds = pos_preds - neg_preds\n",
    "\n",
    "                # Update the data\n",
    "                row = np.concatenate([encoding,obj_encode,preds])\n",
    "                rows.append(row)\n",
    "                pairs.append([entity,each_obj])\n",
    "                \n",
    "            else:\n",
    "                #print('Entity ' + obj_encode + ' has no encoding!')\n",
    "                bad_encodings.append(each_obj)\n",
    "        except:\n",
    "            #print(\"didn't work for \"+ str(entity) + \", \" + str(each_obj))  \n",
    "            bad_encodings.append(each_obj)\n",
    " \n",
    "    rows = np.array(rows)\n",
    "    pairs = np.array(pairs)\n",
    "    if rows.shape[0] == 0:\n",
    "        return({'df':np.NaN,'bad_encodings':np.NaN})\n",
    "    df = pd.concat([pd.DataFrame(pairs),pd.DataFrame(rows)],ignore_index=True,axis=1)\n",
    "    col_names = ['EntityA','EntityB']+['a'+str(x) for x in range(300)]+['b'+str(x) for x in range(300)]+['p'+str(x) for x in range(5)]\n",
    "    df.columns = col_names\n",
    "    return({'df':df,'bad_encodings':bad_encodings})\n",
    "\n",
    "entity = \"Q107507\"\n",
    "test = build_rows(entity)['df']\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running build_rows on all the entities to generate the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = pd.DataFrame([])\n",
    "bad_encodings = []\n",
    "for entity in entities:\n",
    "    rows = build_rows(entity)\n",
    "    try:\n",
    "        all_rows = pd.concat([all_rows,rows['df']])\n",
    "        [bad_encodings.append(x) for x in rows['bad_encodings']]\n",
    "    except:\n",
    "        print ('Entity: '+entity+ ' failed!')\n",
    "        bad_encodings.append(entity)\n",
    "        \n",
    "bad_encodings = np.unique(np.array(bad_encodings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_rows.to_csv('EncodedDataLrg.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode all entities and save as pickle file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Encoded = {entity:encode_entity(entity) for entity in entities}\n",
    "pickle.dump( Encoded, open( \"embeddings/entity_embeddings_lrg.pkl\", \"wb\" ) )\n",
    "pickle.dump( bad_encodings, open( \"embeddings/bad_entities_lrg.pkl\", \"wb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Margaret of Foix',\n",
       " 'Frederik, Crown Prince of Denmark',\n",
       " 'George Windsor, Earl of St Andrews',\n",
       " 'Henry I of England',\n",
       " 'Margrethe II of Denmark',\n",
       " 'Henry II of England',\n",
       " 'Helen of the Palatinate',\n",
       " 'Robert VII, Lord of BÃ\\x83Â©thune',\n",
       " 'Darius III',\n",
       " 'Shigeko Higashikuni',\n",
       " 'Kazuko Takatsukasa',\n",
       " 'Elizabeth Juliana of Schleswig-Holstein-SÃ\\x83Â¸nderburg-Nordborg',\n",
       " 'Isabel Madalena de Hesse-Darmstadt',\n",
       " 'Margaret of Valois, Countess of Blois',\n",
       " 'YelÃ\\x83Â¼ Bei',\n",
       " 'Groucho Marx',\n",
       " 'Prince Kusakabe',\n",
       " 'Julius Caesar',\n",
       " 'Frederick William I of Prussia',\n",
       " 'Lengvenis',\n",
       " 'Henry the Lion',\n",
       " 'Joseph I of Portugal',\n",
       " 'Sancho I of Portugal',\n",
       " 'Simeon Saxe-Coburg-Gotha',\n",
       " 'Queen Elizabeth The Queen Mother',\n",
       " 'Charles Stuart, 1st Earl of Lennox',\n",
       " 'Prince Oscar Bernadotte',\n",
       " 'Jamie Lee Curtis',\n",
       " 'Sibylle of Baden',\n",
       " 'Beatrice of Baden',\n",
       " 'Anna of Brunswick-LÃ\\x83Â¼neburg',\n",
       " 'Joanna of Aragon, Queen of Naples',\n",
       " 'Christine Marie of France',\n",
       " 'Ferdinand I of Portugal',\n",
       " 'Carrie Fisher',\n",
       " 'Princess Anna Sophie of Schwarzburg-Rudolstadt',\n",
       " 'Princess Nanyang',\n",
       " 'David Armstrong-Jones, Viscount Linley',\n",
       " \"Margherita de' Medici\",\n",
       " 'Prince Friedrich Sigismund of Prussia',\n",
       " 'Sophie of Mecklenburg',\n",
       " 'Tamaki',\n",
       " 'YelÃ\\x83Â¼ Lihu',\n",
       " 'Agnes of Meissen',\n",
       " 'Gunda von Savigny',\n",
       " 'Zaynab bint Ali',\n",
       " 'Princess Antoinette, Baroness of Massy',\n",
       " 'Leopold V, Archduke of Austria',\n",
       " 'Georg von Habsburg',\n",
       " 'Archduchess Clementina of Austria',\n",
       " 'Robert, Archduke of Austria-Este',\n",
       " 'Husayn ibn Ali',\n",
       " 'Nelson Rockefeller',\n",
       " 'Prince Charles of Luxembourg',\n",
       " 'Louis, Dauphin of France',\n",
       " 'Louis, Dauphin of France, Duke of Burgundy',\n",
       " 'Archduke Rudolf of Austria',\n",
       " 'Archduke Carl Ludwig of Austria',\n",
       " 'Countess Palatine Irmengard of the Rhine',\n",
       " 'Berengaria of Barcelona',\n",
       " 'Princess Sophie of Liechtenstein',\n",
       " 'Catherine of Austria',\n",
       " 'Senhime',\n",
       " 'Princess Alice of Battenberg',\n",
       " 'Zainab bint Muhammad',\n",
       " 'Victoria, Princess Royal',\n",
       " 'Damian DamiÃ\\x84â\\x84¢cki',\n",
       " 'Mary Woodville',\n",
       " 'Prince Henry, Duke of Gloucester',\n",
       " 'Judita GrojÃ\\x84Â\\x8dskÃ\\x83Â¡',\n",
       " 'Klara Augusta Brunswick-Luneburg',\n",
       " 'John Quincy Adams',\n",
       " 'Amalie of the Palatinate',\n",
       " 'William III, Count of Toulouse',\n",
       " 'Pons, Count of Toulouse',\n",
       " 'Constance of France, Countess of Toulouse',\n",
       " 'Anne of Great Britain',\n",
       " 'Michael Douglas',\n",
       " 'Alexander III of Russia',\n",
       " 'Philip of Swabia',\n",
       " 'Charles II of England',\n",
       " 'Sophie Hedwig of Brunswick-WolfenbÃ\\x83Â¼ttel',\n",
       " 'GarcÃ\\x83Â\\xada Ã\\x83Â\\x8dÃ\\x83Â±iguez of Pamplona',\n",
       " 'Hans Albert Einstein',\n",
       " 'Vilhelmine Augusta af Slesvig-Holsten-Nordborg-PlÃ\\x83Â¸n',\n",
       " 'Eudokia Angelina',\n",
       " 'Anna Komnene Angelina',\n",
       " 'Judith of Baden',\n",
       " 'Mechtild of Baden',\n",
       " 'John Stewart, 1st Earl of Atholl',\n",
       " 'Alexander II of Epirus',\n",
       " 'Rohese of Monmouth',\n",
       " 'Dorothea of Saxony',\n",
       " 'Valentina Visconti, Duchess of OrlÃ\\x83Â©ans',\n",
       " 'Frederick the Fair',\n",
       " 'Dorothy Percy, Countess of Northumberland',\n",
       " 'Michael Landmann',\n",
       " 'William III of the Netherlands',\n",
       " 'James VII and II',\n",
       " 'Beatrice of Provence']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key[x] for x in all_rows['EntityA'].unique()[0:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64950, 607)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
