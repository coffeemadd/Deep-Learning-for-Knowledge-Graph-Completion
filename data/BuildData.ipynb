{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert embeddings into raw data\n",
    "Use the following to convert word embedding using's FaceBooks's fastText into raw data for a standard machine learning model. We are assuming a closed world solution with only positive and negative values for our 5 predicates - isChild, isSpouse, isFather, isMother, and isSibling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn as sk \n",
    "import scipy as sci \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the pickle files containing the word encodings and the entity map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = pickle.load(open('embeddings/word_vectors.pkl','rb'))\n",
    "key = pickle.load(open('processed/entities_map.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the files containing the positive and negative triplets of (entity,predicate,entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 4), (6090, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = dict.fromkeys(range(4), lambda s: s.decode('utf-8'))\n",
    "pos = np.loadtxt(r'processed/positiveTriplets.txt',delimiter=' ',dtype=str, converters=converter)\n",
    "neg = np.loadtxt(r'processed/negativeTriplets.txt',delimiter=' ',dtype=str, converters=converter)\n",
    "pos.shape,neg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of the entities for looping over all of them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P22': 'isFather',\n",
       " 'P25': 'isMother',\n",
       " 'P26': 'isSpouse',\n",
       " 'P3373': 'isSibling',\n",
       " 'P40': 'isChild'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_key = {'P22':'isFather','P25':'isMother','P26':'isSpouse','P3373':'isSibling','P40':'isChild'}\n",
    "pred_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((406,), (5,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = np.unique(pos[:,0])\n",
    "predicates = np.unique(pos[:,1])\n",
    "entities.shape,predicates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper functions for the main data converting tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    \"\"\"\n",
    "        Encodes a word if found in the word \n",
    "        encoding dictionary; otherwise returns\n",
    "        an empty array.\n",
    "    \"\"\"\n",
    "    if word in words.keys():\n",
    "        return(words[word])\n",
    "    max_len = max([words[x].shape[0] for x in words])\n",
    "    return(np.empty(max_len))\n",
    "\n",
    "def encode_entity(entity):\n",
    "    \"\"\"\n",
    "        Builds an entity encoding by computing\n",
    "        the average of all the word encodings.\n",
    "        Ignores any words that don't have an \n",
    "        encoding in the words_vectors.txt file.\n",
    "    \"\"\"\n",
    "    all_words = np.array([encode_word(x) for x in key[entity].split(' ') if x in words.keys()])\n",
    "    if all_words.shape[0]==0:\n",
    "        return(np.array([np.NaN]*300,dtype=float).transpose())\n",
    "    return(all_words.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary Function\n",
    "The build_rows function builds the appropriate rows of numerical data for a given entity. This is done by considering all triplets with this entity as the first object. Then it encodes all entities in this subset, including itself, and concatenates each encoding. Finally it looks as the predicate for each entity-entity pair and assigns 1,0, or -1 depending on if the predicate is true, unknown, or false for that entity-entity pair. The result is a small dataframe of all the rows of this form where the first entity is the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 607)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_rows(entity):\n",
    "    \"\"\"\n",
    "        Builds a dataframe out of the triplets\n",
    "        containing entity as the first object.\n",
    "    \"\"\"\n",
    "    encoding = encode_entity(entity)\n",
    "    temp_pos = pos[pos[:,0]==entity,:]\n",
    "    temp_neg = neg[neg[:,0]==entity,:]\n",
    "    pairs = []\n",
    "    rows = []\n",
    "    bad_encodings = []\n",
    "    for each_obj in np.unique(temp_pos[:,2]):        \n",
    "        try:\n",
    "            obj_encode = encode_entity(each_obj)\n",
    "            \n",
    "            if not all(np.isnan(obj_encode)):\n",
    "                # Get the positive & negative triplet's objects\n",
    "                temp2_pos = temp_pos[temp_pos[:,2]==each_obj,:]\n",
    "                temp2_neg = temp_neg[temp_neg[:,2]==each_obj,:]\n",
    "\n",
    "                # Build outputs from which predicates are in triplets\n",
    "                pos_preds = np.array([x in np.unique(temp2_pos[:,1]) for x in predicates],dtype=int)\n",
    "                neg_preds = np.array([x in np.unique(temp2_neg[:,1]) for x in predicates],dtype=int)\n",
    "                preds = pos_preds - neg_preds\n",
    "\n",
    "                # Update the data\n",
    "                row = np.concatenate([encoding,obj_encode,preds])\n",
    "                rows.append(row)\n",
    "                pairs.append([entity,each_obj])\n",
    "                \n",
    "            else:\n",
    "                #print('Entity ' + obj_encode + ' has no encoding!')\n",
    "                bad_encodings.append(each_obj)\n",
    "        except:\n",
    "            #print(\"didn't work for \"+ str(entity) + \", \" + str(each_obj))  \n",
    "            bad_encodings.append(each_obj)\n",
    " \n",
    "    rows = np.array(rows)\n",
    "    pairs = np.array(pairs)\n",
    "    if rows.shape[0] == 0:\n",
    "        return(np.NaN)\n",
    "    df = pd.concat([pd.DataFrame(pairs),pd.DataFrame(rows)],ignore_index=True,axis=1)\n",
    "    col_names = ['EntityA','EntityB']+['a'+str(x) for x in range(300)]+['b'+str(x) for x in range(300)]+['p'+str(x) for x in range(5)]\n",
    "    df.columns = col_names\n",
    "    return({'df':df,'bad_encodings':bad_encodings})\n",
    "\n",
    "entity = \"Q107507\"\n",
    "test = build_rows(entity)['df']\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running build_rows on all the entities to generate the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = pd.DataFrame([])\n",
    "bad_encodings = []\n",
    "for entity in entities:\n",
    "    rows = build_rows(entity)\n",
    "    if all(df != np.NaN):\n",
    "        all_rows = pd.concat([all_rows,rows['df']])\n",
    "        [bad_encodings.append(x) for x in rows['bad_encodings']]\n",
    "    else:\n",
    "        print ('Entity: '+entity+ ' failed!')\n",
    "        bad_encodings.append(entity)\n",
    "bad_encodings = np.unique(np.array(bad_encodings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_rows.to_csv('EncodedData.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode all entities and save as pickle file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Encoded = {entity:encode_entity(entity) for entity in entities}\n",
    "pickle.dump( Encoded, open( \"embeddings/entity_embeddings.pkl\", \"wb\" ) )\n",
    "pickle.dump( bad_encodings, open( \"embeddings/bad_entities.pkl\", \"wb\") )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
